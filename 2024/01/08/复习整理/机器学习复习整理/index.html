<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>机器学习复习整理 | WinkySpeed's Blog</title><meta name="author" content="WinkySpeed"><meta name="copyright" content="WinkySpeed"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习复习整理 绪论 基本术语 数据 数据集：训练集，测试集 示例&#x2F;样本 属性&#x2F;特征 标记  任务 预测目标 分类（classification）：离散值（比如好瓜，坏瓜） 二分类：好瓜;坏瓜 多分类：冬瓜;南瓜;西瓜  回归（regression）：连续值（比如西瓜的成熟度0.95,0.37） 瓜的成熟度  聚类：无标记信息 有无标记信息 根据训练数据是">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习复习整理">
<meta property="og:url" content="http://example.com/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/index.html">
<meta property="og:site_name" content="WinkySpeed&#39;s Blog">
<meta property="og:description" content="机器学习复习整理 绪论 基本术语 数据 数据集：训练集，测试集 示例&#x2F;样本 属性&#x2F;特征 标记  任务 预测目标 分类（classification）：离散值（比如好瓜，坏瓜） 二分类：好瓜;坏瓜 多分类：冬瓜;南瓜;西瓜  回归（regression）：连续值（比如西瓜的成熟度0.95,0.37） 瓜的成熟度  聚类：无标记信息 有无标记信息 根据训练数据是">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/WinkySpeed%20SP%20-%20Repaired.jpg">
<meta property="article:published_time" content="2024-01-08T01:40:07.000Z">
<meta property="article:modified_time" content="2024-01-11T05:07:08.771Z">
<meta property="article:author" content="WinkySpeed">
<meta property="article:tag" content="复习整理">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/WinkySpeed%20SP%20-%20Repaired.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习复习整理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-11 13:07:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/WinkySpeed%20SP%20-%20Repaired.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">44</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">48</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/33703665_p0.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="WinkySpeed's Blog"><span class="site-name">WinkySpeed's Blog</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习复习整理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-08T01:40:07.000Z" title="发表于 2024-01-08 09:40:07">2024-01-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-11T05:07:08.771Z" title="更新于 2024-01-11 13:07:08">2024-01-11</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习复习整理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="机器学习复习整理">机器学习复习整理</h1>
<h1 id="绪论">绪论</h1>
<h2 id="基本术语">基本术语</h2>
<h3 id="数据">数据</h3>
<p>数据集：训练集，测试集</p>
<p>示例/样本</p>
<p>属性/特征</p>
<p>标记</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108095646639.png" class title="image-20240108095646639">
<h3 id="任务">任务</h3>
<h4 id="预测目标">预测目标</h4>
<p><strong>分类</strong>（classification）：离散值（比如好瓜，坏瓜）</p>
<p>二分类：好瓜;坏瓜</p>
<p>多分类：冬瓜;南瓜;西瓜</p>
<p><br></p>
<p><strong>回归</strong>（regression）：连续值（比如西瓜的成熟度0.95,0.37）</p>
<p>瓜的成熟度</p>
<p><br></p>
<p><strong>聚类</strong>：无标记信息</p>
<h4 id="有无标记信息">有无标记信息</h4>
<p>根据训练数据是否拥有标记信息，学习任务大致分为两大类：监督学习，无监督学习</p>
<p>监督学习：分类、回归</p>
<p>无监督学习：聚类</p>
<p>半监督学习：两者结合</p>
<h3 id="泛化能力">泛化能力</h3>
<p>机器学习的目标是使得学到的模型能很好的适用于“新样本”，而不仅仅是训练集合，我们称模型适用于新样本的能力为<strong>泛化</strong>(generalization)能力</p>
<p>通常假设样本空间中的样本服从一个未知分布<span class="math inline">\(D\)</span>，样本从这个分布中独立获得，即“<strong>独立同分布</strong>”(i.i.d)</p>
<p>一般而言训练样本越多越有可能通过学习获得强泛化能力的模型</p>
<h2 id="假设空间">假设空间</h2>
<p>把学习的过程看做一个在所有假设组成的空间中搜索的过程，搜索目标是找到与训练集“匹配”的假设，即能够将训练集中的瓜正确判断的假设</p>
<p>假设的表示一旦确定，假设空间及其规模大小就确定了</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108100313147.png" class title="image-20240108100313147">
<p>如图，色泽有2种，根蒂有3种，敲声有3种</p>
<p>但是每一个都加了个1：取任意值，也就是不定</p>
<p>最后加的1：空集，啥都不是</p>
<p><span class="math inline">\(3*3*4+1=49\)</span></p>
<h2 id="归纳偏好">归纳偏好</h2>
<p>学习过程中对某种类型假设的偏好称作<strong>归纳偏好</strong></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108101746385.png" class title="image-20240108101746385">
<p>归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或“价值观”</p>
<p>“奥卡姆剃刀”是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，选<strong>最简单</strong>的那个”</p>
<p>具体的现实问题中，<strong>学习算法本身所做的假设是否成立</strong>，也即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能</p>
<h3 id="nofreelunch">NoFreeLunch</h3>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108101917506.png" class title="image-20240108101917506">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108101931043.png" class title="image-20240108101931043">
<h2 id="发展历程">发展历程</h2>
<p>推理期</p>
<p>知识期</p>
<p>学习期</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108102016375.png" class title="image-20240108102016375">
<h3 id="对应学派">对应学派</h3>
<p>符号主义学习：决策树和基于逻辑的学习</p>
<p>连接主义学习：基于神经网络</p>
<p>统计学习：支持向量机和核方法</p>
<p>连接主义学习：深度学习</p>
<h1 id="模型评估与选择">模型评估与选择</h1>
<h2 id="经验误差与过拟合">经验误差与过拟合</h2>
<h3 id="错误率和误差">错误率和误差</h3>
<p>错误率：错分样本的占比</p>
<p>m个样本里有a个样本分类错误，<span class="math inline">\(E=\dfrac{a}{m}\)</span></p>
<p>精度：1-错误率</p>
<p>误差：样本真实输出与预测输出之间的差异</p>
<p>经验（训练）误差：训练集上</p>
<p>测试误差：训练集</p>
<p>泛化误差：除训练集以外的所有样本</p>
<p><br></p>
<p>由于事先并不知道新样本的特征，我们只能努力使经验误差最小化</p>
<p>很多时候虽然能在训练集上做到分类错误率为零，但多数情况下这样的学习器并不好</p>
<h3 id="过拟合">过拟合</h3>
<p>学习器把训练样本学习的“太好”，将训练样本本身的特点当做所有样本的一般性质，导致泛化性能下降</p>
<p>解决方法：</p>
<p>优化目标加正则项</p>
<p>early stop</p>
<h3 id="欠拟合">欠拟合</h3>
<p>对训练样本的一般性质尚未学好</p>
<p>解决方法：</p>
<p>决策树:拓展分支</p>
<p>神经网络:增加训练轮数</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108102634470.png" class title="image-20240108102634470">
<h2 id="评估方法">评估方法</h2>
<p>通过实验测试来对学习器的泛化能力进行评估</p>
<p>现实任务中往往会对学习器的<strong>泛化性能、时间开销、存储开销、可解释性</strong>等方面的因素进行评估并做出选择</p>
<p>我们假设测试集是从样本真实分布中独立采样获得，将测试集上的“<strong>测试误差</strong>”作为泛化误差的近似，所以测试集要和训练集中的样本尽量<strong>互斥</strong></p>
<p>对数据集进行适当处理，分为训练集S和测试集T</p>
<h3 id="留出法">留出法</h3>
<p>直接将数据集划分为两个互斥集合</p>
<p>训练/测试集划分要尽可能保持数据分布的一致性</p>
<p>一般若干次随机划分、<strong>重复实验取平均值</strong></p>
<p>训练/测试样本比例通常为2:1~4:1</p>
<h3 id="交叉检验法">交叉检验法</h3>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108103104455.png" class title="image-20240108103104455">
<p>将数据集D划分为k个子集同样存在多种划分方式，为了减小因样本划分不同而引入的差别，k折交叉验证通常随机使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，例如常见的“10次10折交叉验证”</p>
<h4 id="留一法">留一法</h4>
<p>留一法是交叉检验法的特例，若有m个样本：</p>
<p>令<span class="math inline">\(k=m\)</span>，则每次只剩下一个样本</p>
<p>不受随机样本划分方式的影响</p>
<p>结果往往比较准确</p>
<p>当数据集比较大时，计算开销难以忍受</p>
<h3 id="自助法">自助法</h3>
<p>有放回采样</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108103430743.png" class title="image-20240108103430743">
<p>实际模型与预期模型都使用<span class="math inline">\(m\)</span>个训练样本</p>
<p>约有<span class="math inline">\(\dfrac{1}{3}\)</span>的样本没在训练集中出现</p>
<p>从初始数据集中产生多个不同的训练集，对<strong>集成学习</strong>有很大的好处</p>
<p>自助法在数据集<strong>较小</strong>、难以有效划分训练/测试集时很有用</p>
<p>由于改变了数据集分布可能引入估计偏差，在数据量足够时，留出法和交叉验证法更常用。</p>
<h2 id="性能度量">性能度量</h2>
<p>性能度量是衡量模型泛化能力的评价标准，反映了任务需求；使用不同的性能度量往往会导致不同的评判结果</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108103639254.png" class title="image-20240108103639254">
<p>任务分为连续和离散，即回归和分类</p>
<h3 id="均方误差">均方误差</h3>
<p><strong>回归任务</strong>最常用的性能度量是“<strong>均方误差</strong>”
<span class="math display">\[
E(f;D)=\frac{1}{m}\sum_{i=1}^m(f(x_i)-y_i)^2
\]</span></p>
<h3 id="错误率和精度">错误率和精度</h3>
<p><strong>分类任务</strong>，错误率和精度是最常用的两种性能度量</p>
<p>错误率：分错样本占样本总数的比例</p>
<p>精度：分对样本占样本总数的比率</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108103917094.png" class title="image-20240108103917094">
<h3 id="查准率和查全率">查准率和查全率</h3>
<p>我们更关注的是“挑出来的西瓜有多少是好瓜”</p>
<p>对二分类问题，可以将样例根据真实类别与学习器预测类别的组合，分为TP,
FP, TN, FN</p>
<p>TP：真正例，即预测是正，实际也是正</p>
<p>FP：假正例，即预测是正，实际是反</p>
<p>TN：真反例，即预测是反，实际也是反</p>
<p>FN：假反例，即预测是反，实际是正</p>
<p>TP+FP+TN+FN = 样例总数</p>
<p>列出对应的混淆矩阵：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108104349865.png" class title="image-20240108104349865">
<p>查准率（Precision）：<span class="math inline">\(P=\dfrac{TP}{TP+FP}\)</span></p>
<p>查全率（Recall）：<span class="math inline">\(R=\dfrac{TP}{TP+FN}\)</span></p>
<p>理解查准率：预测<strong>对的</strong>正样本占全部<strong>预测的正样本</strong>的比例，即分母为TP+FP</p>
<p>理解查全率：预测<strong>对的</strong>正样本占<strong>全部正样本</strong>的比例，即分母为TP+FN</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108104949383.png" class title="image-20240108104949383">
<p>准确率就是所有预测正确的样本占全部样本的比例</p>
<p><span class="math inline">\(Acc=\dfrac{TP+TN}{m}\)</span></p>
<p>下图展示了采用不同的阈值，最后的结果是不一样的</p>
<p>比如第一行，1代表阈值取大于0.9，则没有一个查出来是正样本，每一个样本都预测为负样本</p>
<p>第二行，1代表阈值大于0.8，所以0.9这个点就查出来是正样本，其他都认为是负样本，所以Precision查准率为1/1，Recall查全率为1/4</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108105100592.png" class title="image-20240108105100592">
<h3 id="p-r曲线">P-R曲线</h3>
<p>Precision查准率和Recall查全率曲线</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108110524203.png" class title="image-20240108110524203">
<p>平衡点：P=R</p>
<p>比平衡点更常用的是F1度量：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108111339183.png" class title="image-20240108111339183">
<p>比F1更一般的形式：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108111359607.png" class title="image-20240108111359607">
<h3 id="roc与auc">ROC与AUC</h3>
<p>比较ROC曲线的面积，就是AUC</p>
<p>越大越好</p>
<h4 id="roc">ROC</h4>
<p>ROC：受试者工作特征</p>
<p>以“假正例率”为横轴，“真正例率”为纵轴可得到ROC曲线，全称“受试者工作特征”</p>
<h4 id="真正率和假正率">真正率和假正率</h4>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108111855359.png" class title="image-20240108111855359">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108111910839.png" class title="image-20240108111910839">
<p>真正率（TPR）和查全率一样，都是预测正确的正样本占全部正样本的比例，<span class="math inline">\(TPR=\dfrac{TP}{TP+FN}\)</span></p>
<p>假正率（FPR）类似于真正率，是预测正确的负样本占全部负样本的比例，<span class="math display">\[FPR=\dfrac{FP}{FP+TN}\]</span></p>
<h4 id="auc">AUC</h4>
<p>比较ROC曲线的面积，AUC</p>
<p>图一乐就好</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108112455143.png" class title="image-20240108112455143">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108112707406.png" class title="image-20240108112707406">
<h3 id="代价敏感错误率">代价敏感错误率</h3>
<p>代价矩阵</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108113211120.png" class title="image-20240108113211120">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108113157111.png" class title="image-20240108113157111">
<p>代价曲线</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108113221282.png" class title="image-20240108113221282">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108113247498.png" class title="image-20240108113247498">
<h2 id="比较检验">比较检验</h2>
<p>二项检验</p>
<p>t检验</p>
<p>交叉验证t检验</p>
<p>5*2交叉验证法</p>
<p>McNemar检验</p>
<p>Friedman检验</p>
<p>Nemenyi后续检验</p>
<h2 id="偏差与方差">偏差与方差</h2>
<p>除了通过实验对学习算法估计泛化性能，还希望了解“为什么”具有这样的性能</p>
<p>偏差-方差分解是解释学习算法泛化性能的重要工具</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108113601092.png" class title="image-20240108113601092">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108113643284.png" class title="image-20240108113643284">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108113651032.png" class title="image-20240108113651032">
<p><strong>泛化误差</strong><span class="math inline">\(E(f;D)\)</span>可以分解为<strong>偏差</strong><span class="math inline">\(bias^2(x)\)</span>、<strong>方差</strong><span class="math inline">\(var(x)\)</span>与<strong>噪声</strong><span class="math inline">\(\varepsilon^2\)</span>之和，即 <span class="math display">\[
E(f;D)=bias^2(x)+var(x)+\varepsilon^2
\]</span></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108114033643.png" class title="image-20240108114033643">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108114101967.png" class title="image-20240108114101967">
<h1 id="线性模型">线性模型</h1>
<h2 id="基本形式">基本形式</h2>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240108114331016.png" class title="image-20240108114331016">
<p>把系数<span class="math inline">\(\omega\)</span>和自变量x写成向量的形式</p>
<p>简单，是后续模型的基础模型，可解释性好</p>
<h2 id="一元线性回归">一元线性回归</h2>
<p>这个推导要掌握，考试考</p>
<p>推导过程：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240109225343491.png" class title="image-20240109225343491">
<p>有<span class="math inline">\(m\)</span>个样本</p>
<p>分清楚预测值<span class="math inline">\(wx_i+b\)</span>和样本实际值<span class="math inline">\(y_i\)</span></p>
<p>推导的整体思路就是最小二乘法，<span class="math inline">\(b\)</span>对应的<span class="math inline">\(a_0\)</span>，<span class="math inline">\(w\)</span>对应的<span class="math inline">\(a_1\)</span></p>
<p>求对<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>分别求偏导数，使之等于0</p>
<p>只是最后解方程有些麻烦，<span class="math inline">\(w\)</span>化简的时候分子分母同时除以<span class="math inline">\(m\)</span></p>
<p>先解出<span class="math inline">\(w\)</span>，再带回去求<span class="math inline">\(b\)</span></p>
<h2 id="多元线性回归">多元线性回归</h2>
<p>这个推导不需要完全掌握，只用大致记得怎么做就好</p>
<p>公式推导：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240109230058750.png" class title="image-20240109230058750">
<p>有<span class="math inline">\(m\)</span>个样本</p>
<p>在实际上的样本矩阵<span class="math inline">\(X\)</span>最右端添加一列1</p>
<p>中途涉及到矩阵的迹，以及矩阵求导</p>
<h2 id="对数几率回归">对数几率回归</h2>
<h3 id="二分类任务">二分类任务</h3>
<p>对二分类来说，输出就是0或1</p>
<p>输入进模型，输出一个z，想办法把z映射到0或1</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110215706080.png" class title="image-20240110215706080">
<p>用到的函数是<span class="math inline">\(y=\dfrac{1}{1+e^{-z}}\)</span>，名叫对数几率函数，也是“Sigmoid函数”，这个函数有个非常有意思的性质：<span class="math inline">\(f&#39;(x)=f(x)\cdot(1-f(x))\)</span></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110215810213.png" class title="image-20240110215810213">
<p>把<span class="math inline">\(z=w^Tx+b\)</span>带入，得到<span class="math inline">\(y=\dfrac{1}{1+e^{-w^Tx-b}}\)</span></p>
<p>即<span class="math inline">\(\ln{\dfrac{y}{1-y}}=z=w^Tx+b\)</span></p>
<p><span class="math inline">\(\dfrac{y}{1-y}\)</span>是几率，反应了<span class="math inline">\(x\)</span>作为正例的相对可能性</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110220543965.png" class title="image-20240110220543965">
<p><br></p>
<p>注意：虽然叫做“对数几率<strong>回归</strong>”，但他不是连续的问题，因为他是<strong>二分类的问题</strong></p>
<p>对数几率回归虽然叫回归，但是一个二分类的线性模型，是把连续的z投影到0和1上</p>
<p><br></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110220647555.png" class title="image-20240110220647555">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110220700433.png" class title="image-20240110220700433">
<p>这些推导，要注意最开始如何变换的</p>
<p><span class="math inline">\(\ln{\dfrac{y}{1-y}}\)</span>，因为<span class="math inline">\(y\)</span>只能取0或1，所以<span class="math inline">\(y\)</span>就可以看做是<span class="math inline">\(P\{\hat{y}=1|x\}\)</span>的概率，即取值为<span class="math inline">\(x\)</span>时预测值等于1的概率，<span class="math inline">\(1-y\)</span>同理</p>
<p>条件概率，求最大值，就是极大似然</p>
<p>我们为什么不和之前的线性模型一样，用均方误差最小化，最小二乘法？答案是<span class="math inline">\((\dfrac{1}{1+e^{-(w^Tx+b)}}-y)^2\)</span>不是凸函数，因此我们解对率回归用<strong>极大似然法</strong></p>
<p>极大似然法基本的思想就是<span class="math inline">\(P(真是+样本)\cdot
P(预测为+样本)+P(真是-样本)\cdot P(预测为-样本)\)</span>最大</p>
<p>极大似然法通常取对数，因为概率是个很小的值，<strong>很小的值连乘可能会造成浮点数的下溢</strong>，取对数就会变成加法
<span class="math display">\[
\begin{align*}
l(w,b)&amp;=P(真是+样本)\cdot P(预测为+样本)+P(真是-样本)\cdot
P(预测为-样本)\\
&amp;=y\cdot \frac{1}{1+e^{-(w^Tx+b)}}+(1-y)\cdot
(1-\frac{1}{1+e^{-(w^Tx+b)}})\\
我们把w和b写成\beta^T：&amp;=y\cdot \frac{e^{\beta^T x}}{e^{\beta^T
x}+1}+(1-y)\cdot (1-\frac{e^{\beta^T x}}{e^{\beta^T x}+1})\\
&amp;=y\cdot \frac{e^{\beta^T x}}{e^{\beta^T x}+1}+(1-y)\cdot
\frac{1}{e^{\beta^T x}+1}\\\\
现在取\ln，再求最大：\ln l(w,b)&amp;=\ln \frac{ye^{\beta^T
x}+1-y}{e^{\beta^T x}+1}\\
&amp;=\ln(ye^{\beta^T x}+1-y)-\ln(e^{\beta^T x}+1)\\\\
y的取值只可能是0或1：\\
y=1时：&amp;=\beta^T x-\ln(e^{\beta^T x}+1)\\
y=0时：&amp;=0-\ln(e^{\beta^T x}+1)\\\\
把这两种情况结合起来：&amp;=y\cdot \beta^T x-\ln(e^{\beta^T x}+1)
\end{align*}
\]</span></p>
<p>最后就是最大化<span class="math inline">\(y\cdot \beta^T
x-\ln(e^{\beta^T x}+1)\)</span></p>
<p>也可以加个负号取最小：<span class="math inline">\(\min(-y\cdot
\beta^T x+\ln(e^{\beta^T x}+1))\)</span></p>
<h2 id="线性判别学习">线性判别学习</h2>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110221527330.png" class title="image-20240110221527330">
<p>线性模型，如果是分类，就找一条直线分开两类样本点；如果是回归，就拿一条直线尽可能接近这些样本点</p>
<p>用另外一种思想：线性判别分析</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110221943808.png" class title="image-20240110221943808">
<p>如果用线性判别分析的<strong>思想</strong>，我们就不再拿一条直线给两类样本点分开，因为在高维可能会不好分，那么我们就找到一个方向，这个方向称为<span class="math inline">\(w\)</span>，我们让所有样本往这个方向做投影，做完投影以后，我们希望同一类样本点在<span class="math inline">\(w\)</span>方向上的投影<strong>尽可能聚集</strong>，不同类样本，我们希望他们<strong>尽可能隔开</strong></p>
<p>先看投影运算：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110222236247.png" class title="image-20240110222236247">
<p>计算<span class="math inline">\(b\)</span>在<span class="math inline">\(a\)</span>上的投影，计算过程就是这样，大小为<span class="math inline">\(|\vec{b}|\cos\theta\)</span>，方向为<span class="math inline">\(\vec{a}\)</span>的方向，所以最右边<span class="math inline">\(\dfrac{\vec{a}}{|\vec{a}|}\)</span>是<span class="math inline">\(\vec{a}\)</span>的方向</p>
<p>我们同时乘一个<span class="math inline">\(|\vec{a}|\)</span>再除一个<span class="math inline">\(|\vec{a}|\)</span>，不改变原式的值，于是这个投影计算就变成<span class="math inline">\(|\vec{a}|\cdot|\vec{b}|\cos\theta\cdot\dfrac{\vec{a}}{|\vec{a}||\vec{a}|}\)</span>，就变成了<span class="math inline">\(\vec{a}\)</span>和<span class="math inline">\(\vec{b}\)</span>的内积乘以<span class="math inline">\(\dfrac{\vec{a}}{|\vec{a}||\vec{a}|}\)</span>，写成向量乘法就是<span class="math inline">\(a^Tb\)</span></p>
<p><br></p>
<p>用方差刻画波动情况，也就是聚合程度</p>
<p>如果数据是一维的，就是算方差；如果数据是多维的，就是算均方差矩阵</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110223408659.png" class title="image-20240110223408659">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110223423549.png" class title="image-20240110223423549">
<h3 id="推广到多个类">推广到多个类</h3>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110223440965.png" class title="image-20240110223440965">
<h2 id="多分类问题">多分类问题</h2>
<p>拆解法：将一个多分类任务拆分为若干个二分类任务求解</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110223633793.png" class title="image-20240110223633793">
<p>OvO：C1C2训一个，C1C3训一个，C1C4训一个，C2C3训一个，C2C4训一个，C3C4训一个，总共<span class="math inline">\(C_N^2\)</span>个；最后看结果谁输出最多，就分类为谁，比如这里C3出现最多，判断为C3</p>
<p>OvR：把其中的一类作为正样本，其他所有类当做负样本训练第一次</p>
<h1 id="支持向量机">支持向量机</h1>
<h2 id="间隔与支持向量">间隔与支持向量</h2>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110224329388.png" class title="image-20240110224329388">
<p>这些黑线虽然都能把样本分开，但极其容易产生误判，因为黑线稍微扭动一点，就会误判</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110224401423.png" class title="image-20240110224401423">
<p>一看，就是红的更好，因为能分的开，留出了犯错的空间，红线稍微扭动也不影响分类的结果</p>
<p>有了鲁棒性（容错率），容忍性高，泛化能力强</p>
<p><br></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110224758370.png" class title="image-20240110224758370">
<p>比如我们现在已经有了这个红线的方向<span class="math inline">\(w\)</span>，我们把这个红线平移，移动到边界处，在边界处的样本就定义为“<strong>支持向量</strong>”，后续优化的过程与其他点都没关系，只与支持向量有关</p>
<p>两个超平面之间的距离：法向量</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110225038312.png" class title="image-20240110225038312">
<p>我们可以规定，在超平面上的两个线<span class="math inline">\(w^Tx+b\)</span>一个等于1，一个等于-1，因为我们<strong>只需要<span class="math inline">\(w\)</span>的方向</strong>，把<span class="math inline">\(w\)</span>的值进行<strong>缩放</strong>，一定可以得到结果等于正负1的两条线，与<span class="math inline">\(b\)</span>无关</p>
<p>所以，在边界线1以上，所有的值都大于1，在边界线-1以下，所有的点都小于-1</p>
<p>两个超平面：<span class="math inline">\(w^Tx+b-1=0\)</span>和<span class="math inline">\(w^Tx+b+1=0\)</span>，这两个<span class="math inline">\(b\)</span>是相等的</p>
<p>因此，求这两个超平面之间的距离：<span class="math inline">\(d=\dfrac{2}{||w||}\)</span>，定义为<strong>间隔</strong></p>
<p>我们希望，这个间隔尽可能的大</p>
<p><br></p>
<p><span class="math inline">\(y_i=1\)</span>时，<span class="math inline">\(w^Tx_i+b\ge1\)</span></p>
<p><span class="math inline">\(y_i=-1\)</span>时，<span class="math inline">\(w^Tx_i+b\le-1\)</span></p>
<p>这两个式子左右都同时乘以<span class="math inline">\(y_i\)</span>，就可以化成一个式子：<span class="math inline">\(y_i(w^Tx_i+b)\ge1\)</span>，这个式子就是<strong>条件</strong></p>
<p>支持向量机的模型就是这样，找到两个超平面，使他们的间隔达到最大</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110230339450.png" class title="image-20240110230339450">
<p>求个倒数，再求个平方（为了求导数消掉），求最小值；给定一个条件<span class="math inline">\(y_i(w^Tx_i+b)\ge1\)</span></p>
<p>即，带有不等式约束的条件极值问题</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110230442068.png" class title="image-20240110230442068">
<p>带有不等式约束的条件极值问题，解法很容易</p>
<p>找到对应的目标函数<span class="math inline">\(f(x)\)</span>，不等式约束（小于等于0）<span class="math inline">\(g(x)\)</span>，等式约束<span class="math inline">\(h(x)\)</span></p>
<p>拉格朗日乘值法解决这种问题</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111085544955.png" class title="image-20240111085544955">
<p>只需要满足前4个条件即可，第五个条件在非凸优化问题才用</p>
<ol type="1">
<li>对x求偏导等于0</li>
<li>拉格朗日系数<span class="math inline">\(\lambda\)</span>大于等于0</li>
<li>拉格朗日系数乘约束条件等于0</li>
<li>约束条件小于等于0</li>
</ol>
<p>比如：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111085000260.png" class title="image-20240111085000260">
<p>目标函数<span class="math inline">\(f(x)=\dfrac{1}{2}(w_1^2+w_2^2)\)</span></p>
<p>不等式约束，注意符号问题，我们要的是小于等于0，如果是大于等于0，拉格朗日项的系数就是负的</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111085722478.png" class title="image-20240111085722478">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111085848402.png" class title="image-20240111085848402">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111085856052.png" class title="image-20240111085856052">
<h2 id="对偶问题">对偶问题</h2>
<p>如果不等式约束有<span class="math inline">\(m\)</span>个，就需要讨论<span class="math inline">\(2^m\)</span>量级种情况</p>
<p>因为每个参数<span class="math inline">\(\lambda\)</span>都有两种情况，要么等于0，要么大于0，都要充分讨论</p>
<p>因此引入对偶方法</p>
<p>拉格朗日乘子法：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111084516385.png" class title="image-20240111084516385">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111084730754.png" class title="image-20240111084730754">
<p>求解方法：SMO</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111091046838.png" class title="image-20240111091046838">
<h2 id="核函数">核函数</h2>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111091313747.png" class title="image-20240111091313747">
<p>左图就不能直接找一条直线给两种样本点分开</p>
<p>于是就将样本从原始空间映射到一个<strong>更高维</strong>的特征空间，使得样本在这个特征空间内线性可分</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111091414095.png" class title="image-20240111091414095">
<p>原始空间维度有限，一定存在高维特征空间使样本线性可分</p>
<p>即：把<span class="math inline">\(x\)</span>换成高维的<span class="math inline">\(\phi(x)\)</span></p>
<p>其他还是和SVM求解过程一样</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111091628886.png" class title="image-20240111091628886">
<p>但是高维的<span class="math inline">\(\phi(x)\)</span>做内积，太难算了，开销是很大的</p>
<p>假设我们不需要直接算这两个内积，而是算另外的很好算的<strong>代替</strong>，我甚至都不需要知道原来的高维向量<span class="math inline">\(\phi(x)\)</span>是什么</p>
<p>这就引入了<strong>核函数</strong>的概念</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111092027829.png" class title="image-20240111092027829">
<p>核函数的输入是低维空间的输入，处理的结果就等于在某一个高维空间中两个向量的内积</p>
<p>于是就把高维向量的内积转化为低维空间中对核函数求值</p>
<p>绕过了显式考虑特征映射，以及计算高维内积的困难</p>
<p><br></p>
<p>核函数怎么找？</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111092548766.png" class title="image-20240111092548766">
<p>板书来源：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1gG411f7zX?p=36">核函数</a></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111092528555.png" class title="image-20240111092528555">
<p>这个距离矩阵应该是对称的，半正定</p>
<p>核函数就类似于这个<strong>距离矩阵</strong></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111092720102.png" class title="image-20240111092720102">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111093107364.png" class title="image-20240111093107364">
<p>虽然我们能找到一个K，但是我们不能保证这个K张成的高维空间一定可以让两类样本线性可分</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111093252344.png" class title="image-20240111093252344">
<p>SVM每一步都有理论支持，唯一一步可能带来很多不确定性的，就是核函数的选择</p>
<p>机器学习很多地方都是，一定有一个部分，你不可能找到最优</p>
<p>SVM最好的地方就是，不清楚的地方已经清楚地告诉你了，除了核函数的选择，其他都很清楚</p>
<h1 id="神经网络">神经网络</h1>
<p>第一个神经元数学模型, 即M-P模型</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110231542988.png" class title="image-20240110231542988">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110231608590.png" class title="image-20240110231608590">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110231626694.png" class title="image-20240110231626694">
<h2 id="神经元模型">神经元模型</h2>
<p>神经网络是由具有适应性的<strong>简单单元</strong>组成的广泛<strong>并行互联</strong>的网络，它的组织能够模拟<strong>生物神经系统</strong>对真实世界物体所作出的反应</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240110232646809.png" class title="image-20240110232646809">
<p>一系列输入，乘一个<span class="math inline">\(w\)</span>的权重，进入当前神经元，如果比阈值<span class="math inline">\(\theta\)</span>大，<span class="math inline">\(f(x)\)</span>就产生一个响应</p>
<p>这里用到的激活函数是之前在对率回归用到的“Sigmoid函数”</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111094107642.png" class title="image-20240111094107642">
<p>他有一个很有意思的性质：<span class="math inline">\(f&#39;(x)=f(x)\cdot(1-f(x))\)</span></p>
<h2 id="感知机与多层网络">感知机与多层网络</h2>
<h3 id="感知机">感知机</h3>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111094334855.png" class title="image-20240111094334855">
<p>输出部分就是刚刚看到的MP神经元</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111094409040.png" class title="image-20240111094409040">
<p>感知机学习很简单，引入一个参数<span class="math inline">\(\eta\)</span>，叫做学习率，用于调整权重</p>
<p>举个例子：用单层感知机学习“与”运算</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111095211822.png" class title="image-20240111095211822">
<p>此处激活函数用阶跃函数，大于等于0就取1，小于0就取0</p>
<p>真值表如上，x有3个维度的参数，其中与运算只看第2个维度和第3个维度，不看第一个维度<span class="math inline">\(x^{(0)}\)</span></p>
<p>感知机修改<span class="math inline">\(w\)</span>的式子：<span class="math inline">\(w_i=w_i+\Delta w\)</span>，<span class="math inline">\(\Delta w=\eta (y-\hat{y})x_i\)</span></p>
<p>输入为<span class="math inline">\(x_1\)</span>时，<span class="math inline">\(\hat{y_1}=sgn(w_0x_1^T)=sgn([0,1,1][1,0,0]^T)=sgn(0)=1\)</span>，分错了</p>
<p>更新权重为<span class="math inline">\(w_i=w_i+\Delta w=[0,1,1]+\eta
(y-\hat{y})x_i=[0,1,1]+0.5\times(0-1)(1,0,0)^T=[0,1,1]+[-0.5,0,0]=[-0.5,1,1]\)</span></p>
<p>后面每次同理</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100857141.png" class title="image-20240111100857141">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100902150.png" class title="image-20240111100902150">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100908733.png" class title="image-20240111100908733">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100915265.png" class title="image-20240111100915265">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100923117.png" class title="image-20240111100923117">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100930998.png" class title="image-20240111100930998">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100935525.png" class title="image-20240111100935525">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100940681.png" class title="image-20240111100940681">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111100945743.png" class title="image-20240111100945743">
<p>直到所有样本正确分类为止</p>
<h4 id="局限性">局限性</h4>
<p>若两类模式<strong>线性可分</strong>,
则感知机的学习过程一定会<strong>收敛</strong>；否则感知机的学习过程将会发生震荡</p>
<p>单层感知机的学习能力非常有限,
只能解决<strong>线性可分问题</strong></p>
<h3 id="多层网络">多层网络</h3>
<p>隐层：输入层和输出层之间的层，也叫隐含层</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111101213517.png" class title="image-20240111101213517">
<p>多层网络：包含隐层的网络</p>
<p>前馈网络：神经元之间不存在同层连接，也不存在跨层连接</p>
<p>隐层和输出层神经元也称“功能单元”</p>
<p><br></p>
<p>万有逼近性</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111101543415.png" class title="image-20240111101543415">
<h4 id="误差逆传播算法bp">误差逆传播算法（BP）</h4>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111101908720.png" class title="image-20240111101908720">
<p>先把符号搞清楚</p>
<p>输入的是一个样本的多个维度，如图，这个样本就有<span class="math inline">\(d\)</span>个维度</p>
<p>对应乘不同的权重<span class="math inline">\(v_{ih}\)</span>，输入到隐层</p>
<p>图中的隐层，一个神经元接收到的信号输入和为<span class="math inline">\(\alpha_h=\sum_{i=1}^{d}v_{ih}x_i\)</span></p>
<p>隐层的阈值为<span class="math inline">\(\gamma\)</span></p>
<p>隐层一个神经元的输出为<span class="math inline">\(b_h\)</span></p>
<p>输出层一个神经元的输入为<span class="math inline">\(\beta_j\)</span></p>
<p>输出层的阈值为<span class="math inline">\(\theta\)</span></p>
<p>引进符号是为了后面处理的方便</p>
<p><br></p>
<p>回归问题，考虑的就是<strong>均方误差</strong><span class="math inline">\(E_k=\dfrac{1}{2}\sum_{j=1}^l(\hat{y}_j^k-y_j^k)^2\)</span></p>
<p>如图，输入有<span class="math inline">\(d\)</span>个维度，隐层有<span class="math inline">\(q\)</span>个维度，输出有<span class="math inline">\(l\)</span>个维度</p>
<p>需要学习的参数就是<span class="math inline">\(dq+ql+q+l\)</span></p>
<p><span class="math inline">\(dq\)</span>是输入层和隐层之间权重<span class="math inline">\(v\)</span>的个数，<span class="math inline">\(ql\)</span>是隐层和输出层之间权重<span class="math inline">\(w\)</span>的个数</p>
<p>单独一个<span class="math inline">\(q\)</span>是隐层阈值<span class="math inline">\(\gamma\)</span>的个数，单独一个<span class="math inline">\(l\)</span>是输出层阈值<span class="math inline">\(\theta\)</span>的个数</p>
<p><br></p>
<p>BP算法在每一轮中也用到广义感知机学习规则：<span class="math inline">\(w=w+\Delta w\)</span></p>
<p>所以多层前馈网络也叫“多层感知机”</p>
<p><br></p>
<p>BP算法基于“梯度下降”策略，以目标的负梯度方向对参数进行调整</p>
<p>对误差<span class="math inline">\(E_k\)</span>，给定学习率<span class="math inline">\(\eta\)</span>，有<span class="math inline">\(\Delta w_{hj}=-\eta\dfrac{\partial E_k}{\partial
w_{hj}}\)</span></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111103722403.png" class title="image-20240111103722403">
<p>这个求导过程叫“链式法则”</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111103908410.png" class title="image-20240111103908410">
<p>为了写的方便，设置了<span class="math inline">\(g_j\)</span></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111104124132.png" class title="image-20240111104124132">
<p>学习的轮数：数据集里每一个样本都过一次训练，叫一轮</p>
<h4 id="多层前馈网络表示能力">多层前馈网络表示能力</h4>
<p>只需要一个包含足够多神经元的隐层,
多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数</p>
<p>即“万有逼近性”</p>
<h4 id="多次前馈网络局限">多次前馈网络局限</h4>
<p>神经网络由于强大的表示能力，经常遭遇<strong>过拟合</strong>。表现为：训练误差持续降低，但<strong>测试误差却可能上升</strong></p>
<p>如何设置隐层神经元的个数仍然是个未决问题</p>
<p>实际应用中通常使用“<strong>试错法</strong>”调整神经元个数</p>
<h4 id="缓和过拟合">缓和过拟合</h4>
<p>早停：在训练过程中，若训练误差降低，但验证误差升高，则停止训练</p>
<p>正则化：在误差目标函数中增加一项描述网络复杂程度的部分，例如连接权值与阈值的平方和</p>
<h1 id="贝叶斯分类器">贝叶斯分类器</h1>
<h2 id="贝叶斯决策论">贝叶斯决策论</h2>
<p>贝叶斯决策论，是在概率框架下实施决策的基本理论</p>
<p><span class="math inline">\(\lambda_{ij}\)</span>表示第<span class="math inline">\(j\)</span>类样本误分为第<span class="math inline">\(i\)</span>类样本产生的<strong>损失</strong></p>
<p>如果把一个样本<span class="math inline">\(x\)</span>分到了第<span class="math inline">\(i\)</span>类，这时候的风险就是： <span class="math display">\[
R(c_i|x)=\sum_{j=1}^N\lambda_{ij}P(c_{j}|x)
\]</span> 定义<span class="math inline">\(R(c|x)\)</span>为“<strong>条件风险</strong>”</p>
<p>解释一下，首先<span class="math inline">\(x\)</span>这个样本可能属于很多类别中间的某一个，就用一个概率刻画这件事，即如果<span class="math inline">\(x\)</span>属于第<span class="math inline">\(j\)</span>类，概率就是<span class="math inline">\(P(c_{j}|x)\)</span>，把这个概率定义为“<strong>后验概率</strong>”</p>
<p>乘以对应的风险<span class="math inline">\(\lambda_{ij}\)</span>，再把所有项用<span class="math inline">\(j\)</span>求和，就是最后总风险</p>
<p>在此基础上，把<span class="math inline">\(x\)</span>分到所有类的<span class="math inline">\(R(c|x)\)</span>都能算出来，在此基础上求一个最小值（取风险最小的一个），就是贝叶斯判定准则</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111105800636.png" class title="image-20240111105800636">
<p>注意，<span class="math inline">\(\lambda_{ij}\)</span>是客观存在的，具体来说，如果目标是最小化分类错误率，那么<span class="math inline">\(\lambda_{ij}\)</span>可以写成：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111111654318.png" class title="image-20240111111654318">
<p>若<span class="math inline">\(x\)</span>为第<span class="math inline">\(i\)</span>类，此时的条件风险可以化为： <span class="math display">\[
\begin{align*}
R(c_i|x)&amp;=\sum_{j=1}^N\lambda_{ij}P(c_{j}|x)\\
&amp;=\sum_{j=1,j\ne i}^N1\cdot P(c_{j}|x)\\\\
&amp;=1-P(c_{i}|x)
\end{align*}
\]</span> 即<span class="math inline">\(R(c|x)=1-P(c|x)\)</span></p>
<p>但是后验概率<span class="math inline">\(P(c|x)\)</span>这个条件概率没有办法事先知道，在现实中通常很难获得</p>
<p>所以，机器学习就是为了把<span class="math inline">\(P(c|x)\)</span>找出来（这只是从贝叶斯决策论的角度对机器学习的理解）</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111110926358.png" class title="image-20240111110926358">
<p>判别式模型和生成式模型基本的区别就是，判别式模型拿到了数据集就直接处理，生成式模型是希望把数据集原来的分布求出来</p>
<p><br></p>
<p>根据生成式模型，可以用联合概率分布求得后验概率：<span class="math inline">\(P(c|x)=\dfrac{P(x,c)}{P(x)}\)</span></p>
<p>假设独立同分布，就可以用贝叶斯公式化为： <span class="math display">\[
P(c|x)=\frac{P(c)\cdot P(x|c)}{P(x)}
\]</span></p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111112320667.png" class title="image-20240111112320667">
<h2 id="极大似然估计">极大似然估计</h2>
<p>首先要假设某种概率分布形式，再基于训练样本对参数进行估计</p>
<p>假定<span class="math inline">\(P(x|c)\)</span>具有确定的概率分布形式，并且这个分布被参数<span class="math inline">\(\theta_c\)</span>唯一确定，任务就是利用训练集<span class="math inline">\(D\)</span>估计参数<span class="math inline">\(\theta_c\)</span>，即把数据集背后的分布估计出来</p>
<p>假设<span class="math inline">\(x\)</span>独立同分布</p>
<p><span class="math inline">\(\theta_c\)</span>对于训练集<span class="math inline">\(D\)</span>中第<span class="math inline">\(c\)</span>类样本组成的集合<span class="math inline">\(D_c\)</span>的似然为： <span class="math display">\[
P(D_c|\theta_c)=\prod_{x\in D_c}P(x|\theta_c)
\]</span></p>
<p>对<span class="math inline">\(\theta_c\)</span>进行极大似然估计，但因为小的数连乘问题，会导致浮点数下溢，所以再加一个<span class="math inline">\(\log\)</span>，即对数似然(Log-Likelihood) <span class="math display">\[
LL(\theta_c)=\log P(D_c|\theta_c)=\sum_{x\in D_c}\log P(x|\theta_c)
\]</span> 极大似然估计就是<span class="math inline">\(\hat{\theta}_c=\max LL(\theta_c)\)</span></p>
<p>这就是极大似然法解决问题的框架，是一个方法论</p>
<p>先把似然函数写出来，每一个样本都是独立随机事件，考虑每一个样本都出现，似然最大，连乘换成连加</p>
<p>比如，假设概率密度函数</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111114322784.png" class title="image-20240111114322784">
<h2 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h2>
<p>从贝叶斯公式出发： <span class="math display">\[
P(c|x)=\frac{P(c)\cdot P(x|c)}{P(x)}
\]</span></p>
<p>假设属性相互独立，就可以把<span class="math inline">\(P(x|c)\)</span>写成累乘 <span class="math display">\[
P(c|x)=\frac{P(c)}{P(x)}\cdot \prod_{i=1}^d P(x_i|c)
\]</span> <span class="math inline">\(P(x)\)</span>对所有类别相同，于是不考虑<span class="math inline">\(P(x)\)</span> <span class="math display">\[
h_{nb}(x)=\max P(c)\cdot\prod_{i=1}^d P(x_i|c)
\]</span></p>
<p>估计<span class="math inline">\(P(c)\)</span>：<span class="math inline">\(P(c)=\dfrac{|D_c|}{|D|}\)</span>，用第<span class="math inline">\(c\)</span>个类的样本数除以样本总数，比如现在有100个瓜，有70个是好的，那好瓜就是0.7，坏瓜就是0.3</p>
<p>估计<span class="math inline">\(P(x|c)\)</span>：</p>
<p>对于离散属性，令<span class="math inline">\(D_{c,x_i}\)</span>表示<span class="math inline">\(D_c\)</span>中在第<span class="math inline">\(i\)</span>个属性上取值为<span class="math inline">\(x_i\)</span>的样本组成的集合，则： <span class="math display">\[
P(x_i|c)=\frac{|D_{c,x_i}|}{|D_c|}
\]</span> 举个例子：比如<span class="math inline">\(D_c\)</span>是好瓜的集合，<span class="math inline">\(D_{c,x_i}\)</span>是好瓜里颜色是青绿色的集合</p>
<p>对连续属性，考虑概率密度函数，假定高斯分布：</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111123717296.png" class title="image-20240111123717296">
<p>例子：</p>
<p><img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111123947813.png" class title="image-20240111123947813"></p>
<p>很明显好瓜的概率比坏瓜大，就认为是好瓜</p>
<h2 id="半朴素贝叶斯分类器">半朴素贝叶斯分类器</h2>
<p>朴素贝叶斯分类器的这个“朴素”假设（即特征之间<strong>相互独立</strong>）在许多实际应用中可能并不成立</p>
<p>这就引出“半朴素贝叶斯”</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111124105410.png" class title="image-20240111124105410">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111124123743.png" class title="image-20240111124123743">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111124133660.png" class title="image-20240111124133660">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111124139877.png" class title="image-20240111124139877">
<h1 id="聚类">聚类</h1>
<h2 id="聚类任务">聚类任务</h2>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111124538540.png" class title="image-20240111124538540">
<p>聚类也叫“集簇”</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111124916321.png" class title="image-20240111124916321">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111124927013.png" class title="image-20240111124927013">
<h2 id="性能度量-1">性能度量</h2>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111125040562.png" class title="image-20240111125040562">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130108533.png" class title="image-20240111130108533">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130113871.png" class title="image-20240111130113871">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130123310.png" class title="image-20240111130123310">
<h2 id="距离计算">距离计算</h2>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130131560.png" class title="image-20240111130131560">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130139986.png" class title="image-20240111130139986">
<h2 id="聚类分类">聚类分类</h2>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111125836874.png" class title="image-20240111125836874">
<h3 id="原型聚类">原型聚类</h3>
<h4 id="k均值算法">K均值算法</h4>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130211090.png" class title="image-20240111130211090">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130219029.png" class title="image-20240111130219029">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130230445.png" class title="image-20240111130230445">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130242559.png" class title="image-20240111130242559">
<h4 id="lvq">LVQ</h4>
<p>LVQ假设数据样本带有类别标记，即“有监督学习”</p>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130301234.png" class title="image-20240111130301234">
<h4 id="k均值和lvq">K均值和LVQ</h4>
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130524718.png" class title="image-20240111130524718">
<img src="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/image-20240111130533465.png" class title="image-20240111130533465">
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/">复习整理</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/02/23/%E6%B8%B8%E6%88%8F%E9%80%9A%E5%85%B3%E6%84%9F%E6%83%B3/%E6%98%9F%E9%99%85%E6%8B%93%E8%8D%92%E9%80%9A%E5%85%B3%E6%84%9F%E6%83%B3/" title="星际拓荒通关感想"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">星际拓荒通关感想</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/07/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="计算机图形学复习整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计算机图形学复习整理</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/12/12/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%8E%A5%E5%8F%A3%E6%8A%80%E6%9C%AF%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="接口技术复习整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-12</div><div class="title">接口技术复习整理</div></div></a></div><div><a href="/2023/12/06/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/OOAD%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="OOAD复习整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-06</div><div class="title">OOAD复习整理</div></div></a></div><div><a href="/2023/12/28/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="数值分析复习整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-28</div><div class="title">数值分析复习整理</div></div></a></div><div><a href="/2024/01/01/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="数字图像处理复习整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-01</div><div class="title">数字图像处理复习整理</div></div></a></div><div><a href="/2024/01/07/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="计算机图形学复习整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-07</div><div class="title">计算机图形学复习整理</div></div></a></div><div><a href="/2023/12/30/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6%E7%BC%96%E7%A8%8B%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="计算机硬件编程复习整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-30</div><div class="title">计算机硬件编程复习整理</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/WinkySpeed%20SP%20-%20Repaired.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">WinkySpeed</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">44</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">48</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/WinkySpeed"><i class="fab fa-github"></i><span>GitHub</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">机器学习复习整理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%AA%E8%AE%BA"><span class="toc-number">2.</span> <span class="toc-text">绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%9C%AF%E8%AF%AD"><span class="toc-number">2.1.</span> <span class="toc-text">基本术语</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.1.</span> <span class="toc-text">数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1"><span class="toc-number">2.1.2.</span> <span class="toc-text">任务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E7%9B%AE%E6%A0%87"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">预测目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%89%E6%97%A0%E6%A0%87%E8%AE%B0%E4%BF%A1%E6%81%AF"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">有无标记信息</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B"><span class="toc-number">2.1.3.</span> <span class="toc-text">泛化能力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4"><span class="toc-number">2.2.</span> <span class="toc-text">假设空间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E7%BA%B3%E5%81%8F%E5%A5%BD"><span class="toc-number">2.3.</span> <span class="toc-text">归纳偏好</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nofreelunch"><span class="toc-number">2.3.1.</span> <span class="toc-text">NoFreeLunch</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B"><span class="toc-number">2.4.</span> <span class="toc-text">发展历程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%BA%94%E5%AD%A6%E6%B4%BE"><span class="toc-number">2.4.1.</span> <span class="toc-text">对应学派</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="toc-number">3.</span> <span class="toc-text">模型评估与选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E8%AF%AF%E5%B7%AE%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">3.1.</span> <span class="toc-text">经验误差与过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%94%99%E8%AF%AF%E7%8E%87%E5%92%8C%E8%AF%AF%E5%B7%AE"><span class="toc-number">3.1.1.</span> <span class="toc-text">错误率和误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">3.1.2.</span> <span class="toc-text">过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">3.1.3.</span> <span class="toc-text">欠拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">评估方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%95%99%E5%87%BA%E6%B3%95"><span class="toc-number">3.2.1.</span> <span class="toc-text">留出法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C%E6%B3%95"><span class="toc-number">3.2.2.</span> <span class="toc-text">交叉检验法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%95%99%E4%B8%80%E6%B3%95"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">留一法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A9%E6%B3%95"><span class="toc-number">3.2.3.</span> <span class="toc-text">自助法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F"><span class="toc-number">3.3.</span> <span class="toc-text">性能度量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">3.3.1.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%94%99%E8%AF%AF%E7%8E%87%E5%92%8C%E7%B2%BE%E5%BA%A6"><span class="toc-number">3.3.2.</span> <span class="toc-text">错误率和精度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E5%87%86%E7%8E%87%E5%92%8C%E6%9F%A5%E5%85%A8%E7%8E%87"><span class="toc-number">3.3.3.</span> <span class="toc-text">查准率和查全率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#p-r%E6%9B%B2%E7%BA%BF"><span class="toc-number">3.3.4.</span> <span class="toc-text">P-R曲线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#roc%E4%B8%8Eauc"><span class="toc-number">3.3.5.</span> <span class="toc-text">ROC与AUC</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#roc"><span class="toc-number">3.3.5.1.</span> <span class="toc-text">ROC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9C%9F%E6%AD%A3%E7%8E%87%E5%92%8C%E5%81%87%E6%AD%A3%E7%8E%87"><span class="toc-number">3.3.5.2.</span> <span class="toc-text">真正率和假正率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#auc"><span class="toc-number">3.3.5.3.</span> <span class="toc-text">AUC</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F%E9%94%99%E8%AF%AF%E7%8E%87"><span class="toc-number">3.3.6.</span> <span class="toc-text">代价敏感错误率</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E6%A3%80%E9%AA%8C"><span class="toc-number">3.4.</span> <span class="toc-text">比较检验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="toc-number">3.5.</span> <span class="toc-text">偏差与方差</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F"><span class="toc-number">4.1.</span> <span class="toc-text">基本形式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">4.2.</span> <span class="toc-text">一元线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">4.3.</span> <span class="toc-text">多元线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92"><span class="toc-number">4.4.</span> <span class="toc-text">对数几率回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1"><span class="toc-number">4.4.1.</span> <span class="toc-text">二分类任务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.5.</span> <span class="toc-text">线性判别学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E5%B9%BF%E5%88%B0%E5%A4%9A%E4%B8%AA%E7%B1%BB"><span class="toc-number">4.5.1.</span> <span class="toc-text">推广到多个类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">4.6.</span> <span class="toc-text">多分类问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">5.</span> <span class="toc-text">支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%B4%E9%9A%94%E4%B8%8E%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="toc-number">5.1.</span> <span class="toc-text">间隔与支持向量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-number">5.2.</span> <span class="toc-text">对偶问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">5.3.</span> <span class="toc-text">核函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.1.</span> <span class="toc-text">神经元模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E5%A4%9A%E5%B1%82%E7%BD%91%E7%BB%9C"><span class="toc-number">6.2.</span> <span class="toc-text">感知机与多层网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">6.2.1.</span> <span class="toc-text">感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">6.2.1.1.</span> <span class="toc-text">局限性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E7%BD%91%E7%BB%9C"><span class="toc-number">6.2.2.</span> <span class="toc-text">多层网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E9%80%86%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95bp"><span class="toc-number">6.2.2.1.</span> <span class="toc-text">误差逆传播算法（BP）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E8%83%BD%E5%8A%9B"><span class="toc-number">6.2.2.2.</span> <span class="toc-text">多层前馈网络表示能力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E6%AC%A1%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%E5%B1%80%E9%99%90"><span class="toc-number">6.2.2.3.</span> <span class="toc-text">多次前馈网络局限</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%93%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">6.2.2.4.</span> <span class="toc-text">缓和过拟合</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">7.</span> <span class="toc-text">贝叶斯分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA"><span class="toc-number">7.1.</span> <span class="toc-text">贝叶斯决策论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">7.2.</span> <span class="toc-text">极大似然估计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">7.3.</span> <span class="toc-text">朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">7.4.</span> <span class="toc-text">半朴素贝叶斯分类器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB"><span class="toc-number">8.</span> <span class="toc-text">聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E4%BB%BB%E5%8A%A1"><span class="toc-number">8.1.</span> <span class="toc-text">聚类任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-1"><span class="toc-number">8.2.</span> <span class="toc-text">性能度量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">8.3.</span> <span class="toc-text">距离计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="toc-number">8.4.</span> <span class="toc-text">聚类分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9E%8B%E8%81%9A%E7%B1%BB"><span class="toc-number">8.4.1.</span> <span class="toc-text">原型聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#k%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95"><span class="toc-number">8.4.1.1.</span> <span class="toc-text">K均值算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lvq"><span class="toc-number">8.4.1.2.</span> <span class="toc-text">LVQ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#k%E5%9D%87%E5%80%BC%E5%92%8Clvq"><span class="toc-number">8.4.1.3.</span> <span class="toc-text">K均值和LVQ</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/13/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/2024%E8%93%9D%E6%A1%A5%E6%9D%AF%E8%AE%AD%E7%BB%83/" title="2024蓝桥杯训练">2024蓝桥杯训练</a><time datetime="2024-03-13T02:07:17.000Z" title="发表于 2024-03-13 10:07:17">2024-03-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/02/23/%E6%B8%B8%E6%88%8F%E9%80%9A%E5%85%B3%E6%84%9F%E6%83%B3/%E6%98%9F%E9%99%85%E6%8B%93%E8%8D%92%E9%80%9A%E5%85%B3%E6%84%9F%E6%83%B3/" title="星际拓荒通关感想">星际拓荒通关感想</a><time datetime="2024-02-22T16:59:34.000Z" title="发表于 2024-02-23 00:59:34">2024-02-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/08/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="机器学习复习整理">机器学习复习整理</a><time datetime="2024-01-08T01:40:07.000Z" title="发表于 2024-01-08 09:40:07">2024-01-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/07/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="计算机图形学复习整理">计算机图形学复习整理</a><time datetime="2024-01-07T13:06:54.000Z" title="发表于 2024-01-07 21:06:54">2024-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/01/01/%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%A4%8D%E4%B9%A0%E6%95%B4%E7%90%86/" title="数字图像处理复习整理">数字图像处理复习整理</a><time datetime="2024-01-01T07:03:37.000Z" title="发表于 2024-01-01 15:03:37">2024-01-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By WinkySpeed</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2d-widget/autoload.js"></script></body></html>